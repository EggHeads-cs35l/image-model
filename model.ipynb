{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check versions and devices\n",
    "print(\"python version: \", sys.version)\n",
    "# print(\"tf version: \", tf.__version__)\n",
    "# print(\"devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "(train_ds, test_ds), ds_info = tfds.load(\n",
    "    'cars196',\n",
    "    split=['train[:80%]+test[:80%]', 'train[80%:]+test[80%:]'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "print(\"Num train samples: \", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"Num test samples: \", tf.data.experimental.cardinality(test_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some samples\n",
    "tfds.visualization.show_examples(train_ds, ds_info, rows=1, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "height = width = 224\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x,(height, width)), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x,(height, width)), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "autotune = tf.data.AUTOTUNE\n",
    "batch_size = 128\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, height + 6, width + 6)\n",
    "    image = tf.image.random_crop(image, size=[height, width, 3])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_hue(image, 0.2)\n",
    "    image = tf.image.random_contrast(image, 0.5, 2)\n",
    "    image = tf.image.random_saturation(image, 0, 2)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.cache().map(augment).shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=autotune)\n",
    "test_ds = test_ds.cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(196)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "# save model and history\n",
    "model.save('model.h5')\n",
    "with open(\"hist.json\", \"w\") as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# load model and history\n",
    "model = load_model('model.h5')\n",
    "model_hist = pd.read_json('hist.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average training accuracy: \", model_hist['accuracy'].mean()*100, \"%\")\n",
    "print(\"Average validation accuracy: \", model_hist['val_accuracy'].mean()*100, \"%\")\n",
    "print(\"Average training loss: \", model_hist['loss'].mean())\n",
    "print(\"Average validation loss: \", model_hist['val_loss'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy and val_accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_hist['accuracy'], label='train')\n",
    "plt.plot(model_hist['val_accuracy'], label = 'test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc ='upper left')\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10b546dc9a6492740535e7e359781ac3cb076df88249adf04015d7e9d7b6abb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
